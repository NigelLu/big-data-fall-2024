{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Midterm xl3139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, MinHashLSH, CountVectorizer, Tokenizer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataframe(\n",
    "    dataframe: DataFrame, num_rows_to_show: int = 10, prefix: str = \"\", suffix: str = \"\"\n",
    ") -> None:\n",
    "    prefix = prefix + \":\\n\" if prefix else prefix\n",
    "    suffix = suffix + \"\\n\" if suffix else suffix\n",
    "    print(f\"------------\\n{prefix}\")\n",
    "    dataframe.show(num_rows_to_show)\n",
    "    print(f\"{suffix}------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Spark - Language Models - in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * constants\n",
    "APP_NAME_Q2 = \"Question2\"\n",
    "WORD_TEXT_FILE_PATH = \"./data/*.txt\"\n",
    "\n",
    "READ_IN_DATA_COL = \"value\"\n",
    "PRE_PROCESSED_TEXT_COL = \"processed_text\"\n",
    "TOKENS_COL = \"tokens\"\n",
    "\n",
    "BIGRMAS_LIST_COL = \"bigrams_list\"\n",
    "TRIGRAMS_LIST_COL = \"trigrams_list\"\n",
    "\n",
    "BIGRAM_COL = \"bigram\"\n",
    "TRIGRAM_COL = \"trigram\"\n",
    "\n",
    "BIGRAM_FROM_TRIGRAM_COL = \"bigram_from_trigram\"\n",
    "BIGRAM_FROM_TRIGRAM_COUNT_COL = \"bigram_from_trigram_count\"\n",
    "BIGRAM_COUNT_COL = \"bigram_count\"\n",
    "\n",
    "CONDITIONAL_PROBABILITY_COL = \"condational_probability\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|@@31678741 <p> If...|\n",
      "|@@31680641 <p> In...|\n",
      "|@@31680841 <p> Th...|\n",
      "|@@31682241 <h> Le...|\n",
      "|@@31683241 <h> OE...|\n",
      "|@@31683741 <h> Cl...|\n",
      "|@@31684841 <p> Un...|\n",
      "|@@31685141 <h> Co...|\n",
      "|@@31686141 <h> Cl...|\n",
      "|@@31688541 <h> Wi...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 18:45:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# * create the spark session\n",
    "spark = SparkSession.builder.appName(APP_NAME_Q2).master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# * load the text data\n",
    "df_text = spark.read.text(WORD_TEXT_FILE_PATH).filter(\n",
    "    F.col(READ_IN_DATA_COL).isNotNull() & (F.col(READ_IN_DATA_COL) != \"\")\n",
    ")\n",
    "show_dataframe(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Processed text:\n",
      "\n",
      "+--------------------+\n",
      "|      processed_text|\n",
      "+--------------------+\n",
      "|31678741 p if i p...|\n",
      "|31680641 p in the...|\n",
      "|31680841 p thousa...|\n",
      "|31682241 h lesson...|\n",
      "|31683241 h oecd w...|\n",
      "|31683741 h closin...|\n",
      "|31684841 p until ...|\n",
      "|31685141 h corona...|\n",
      "|31686141 h clorox...|\n",
      "|31688541 h will c...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "------------\n",
      "Tokens:\n",
      "\n",
      "+--------------------+\n",
      "|              tokens|\n",
      "+--------------------+\n",
      "|[31678741, p, if,...|\n",
      "|[31680641, p, in,...|\n",
      "|[31680841, p, tho...|\n",
      "|[31682241, h, les...|\n",
      "|[31683241, h, oec...|\n",
      "|[31683741, h, clo...|\n",
      "|[31684841, p, unt...|\n",
      "|[31685141, h, cor...|\n",
      "|[31686141, h, clo...|\n",
      "|[31688541, h, wil...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * pre-processing\n",
    "# 1. remove all non-alphanumerical characters\n",
    "# 2. remove all blank spaces at the beginning and tail\n",
    "df_text_cleaned = df_text.select(\n",
    "    F.trim(F.lower(F.regexp_replace(F.col(READ_IN_DATA_COL), \"[^0-9a-zA-Z]+\", \" \"))).alias(PRE_PROCESSED_TEXT_COL)\n",
    ")\n",
    "show_dataframe(df_text_cleaned, prefix=\"Processed text\")\n",
    "\n",
    "# * tokenize\n",
    "tokenizer = RegexTokenizer(inputCol=PRE_PROCESSED_TEXT_COL, outputCol=TOKENS_COL, pattern=\"\\\\s+\", gaps=True)\n",
    "df_tokens = tokenizer.transform(df_text_cleaned).drop(PRE_PROCESSED_TEXT_COL)\n",
    "show_dataframe(df_tokens, prefix=\"Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Bigrams:\n",
      "\n",
      "+--------------------+\n",
      "|        bigrams_list|\n",
      "+--------------------+\n",
      "|[31678741 p, p if...|\n",
      "|[31680641 p, p in...|\n",
      "|[31680841 p, p th...|\n",
      "|[31682241 h, h le...|\n",
      "|[31683241 h, h oe...|\n",
      "|[31683741 h, h cl...|\n",
      "|[31684841 p, p un...|\n",
      "|[31685141 h, h co...|\n",
      "|[31686141 h, h cl...|\n",
      "|[31688541 h, h wi...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "------------\n",
      "Trigrams:\n",
      "\n",
      "+--------------------+\n",
      "|       trigrams_list|\n",
      "+--------------------+\n",
      "|[31678741 p if, p...|\n",
      "|[31680641 p in, p...|\n",
      "|[31680841 p thous...|\n",
      "|[31682241 h lesso...|\n",
      "|[31683241 h oecd,...|\n",
      "|[31683741 h closi...|\n",
      "|[31684841 p until...|\n",
      "|[31685141 h coron...|\n",
      "|[31686141 h cloro...|\n",
      "|[31688541 h will,...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * get the bigrams and trigrams\n",
    "bigram = NGram(n=2, inputCol=TOKENS_COL, outputCol=BIGRMAS_LIST_COL)\n",
    "trigram = NGram(n=3, inputCol=TOKENS_COL, outputCol=TRIGRAMS_LIST_COL)\n",
    "df_bigrams = bigram.transform(df_tokens).drop(TOKENS_COL)\n",
    "df_trigrams = trigram.transform(df_tokens).drop(TOKENS_COL)\n",
    "show_dataframe(df_bigrams, prefix=\"Bigrams\")\n",
    "show_dataframe(df_trigrams, prefix=\"Trigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Bigrams Count:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         bigram|count|\n",
      "+---------------+-----+\n",
      "|       in south|  189|\n",
      "|        for low|   14|\n",
      "|    reserve the|   21|\n",
      "|      access to|  524|\n",
      "|        is more|  176|\n",
      "|    emphasise p|    1|\n",
      "|     the strain|   21|\n",
      "|     center are|    2|\n",
      "|  virus spreads|   34|\n",
      "|potential every|    1|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "------------\n",
      "Trigrams Count:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             trigram|count|\n",
      "+--------------------+-----+\n",
      "|     lot of promises|    1|\n",
      "|    at tuesday night|    1|\n",
      "|  selling a quixotic|    1|\n",
      "|     one p klobuchar|    1|\n",
      "|most clearly tuesday|    1|\n",
      "|   visit london four|    1|\n",
      "|qualifier against...|    1|\n",
      "|heading household...|    1|\n",
      "|their greater com...|    1|\n",
      "|most heavily deva...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "------------\n",
      "Top 10 Trigrams:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           trigram|count|\n",
      "+------------------+-----+\n",
      "|           lt p gt| 1928|\n",
      "|      the covid 19| 1718|\n",
      "|            do n t| 1662|\n",
      "|       of covid 19| 1589|\n",
      "|     the spread of| 1196|\n",
      "|           p gt lt| 1037|\n",
      "|     the number of| 1037|\n",
      "|           gt lt p| 1023|\n",
      "|        one of the|  953|\n",
      "|of the coronavirus|  907|\n",
      "+------------------+-----+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# * count the number of occurrences for each bigram and trigram\n",
    "df_bigrams_count = (\n",
    "    df_bigrams.select(F.explode(F.col(BIGRMAS_LIST_COL)).alias(BIGRAM_COL))\n",
    "    .groupBy(BIGRAM_COL)\n",
    "    .count()\n",
    ")\n",
    "df_trigrams_count = (\n",
    "    df_trigrams.select(F.explode(F.col(TRIGRAMS_LIST_COL)).alias(TRIGRAM_COL))\n",
    "    .groupBy(TRIGRAM_COL)\n",
    "    .count()\n",
    ")\n",
    "show_dataframe(df_bigrams_count, prefix=\"Bigrams Count\")\n",
    "show_dataframe(df_trigrams_count, prefix=\"Trigrams Count\")\n",
    "\n",
    "# * top 10 trigrams\n",
    "df_top_ten_trigrams = df_trigrams_count.orderBy(F.col(\"count\").desc()).limit(10)\n",
    "show_dataframe(df_top_ten_trigrams, prefix=\"Top 10 Trigrams\")\n",
    "\n",
    "# * split the trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Bigram from Trigram:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:>                                                      (0 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+-------------------+\n",
      "|           trigram|bigram_from_trigram_count|bigram_from_trigram|\n",
      "+------------------+-------------------------+-------------------+\n",
      "|           lt p gt|                     1928|               lt p|\n",
      "|      the covid 19|                     1718|          the covid|\n",
      "|            do n t|                     1662|               do n|\n",
      "|       of covid 19|                     1589|           of covid|\n",
      "|     the spread of|                     1196|         the spread|\n",
      "|           p gt lt|                     1037|               p gt|\n",
      "|     the number of|                     1037|         the number|\n",
      "|           gt lt p|                     1023|              gt lt|\n",
      "|        one of the|                      953|             one of|\n",
      "|of the coronavirus|                      907|             of the|\n",
      "+------------------+-------------------------+-------------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_bigram_from_trigram = (\n",
    "    df_top_ten_trigrams.withColumn(TOKENS_COL, F.split(F.col(TRIGRAM_COL), \" \"))\n",
    "    .withColumn(\n",
    "        BIGRAM_FROM_TRIGRAM_COL, F.concat_ws(\" \", F.col(TOKENS_COL)[0], F.col(TOKENS_COL)[1])\n",
    "    )\n",
    "    .withColumnRenamed(\"count\", BIGRAM_FROM_TRIGRAM_COUNT_COL)\n",
    "    .drop(TOKENS_COL)\n",
    ")\n",
    "show_dataframe(df_bigram_from_trigram, prefix=\"Bigram from Trigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Joined Dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------------------+------------+\n",
      "|           trigram|bigram_from_trigram|bigram_from_trigram_count|bigram_count|\n",
      "+------------------+-------------------+-------------------------+------------+\n",
      "|      the covid 19|          the covid|                     1718|        1760|\n",
      "|            do n t|               do n|                     1662|        1662|\n",
      "|     the spread of|         the spread|                     1196|        1306|\n",
      "|       of covid 19|           of covid|                     1589|        1621|\n",
      "|        one of the|             one of|                      953|        1491|\n",
      "|           gt lt p|              gt lt|                     1023|        1792|\n",
      "|           p gt lt|               p gt|                     1037|        1936|\n",
      "|of the coronavirus|             of the|                      907|       17484|\n",
      "|           lt p gt|               lt p|                     1928|        1930|\n",
      "|     the number of|         the number|                     1037|        1103|\n",
      "+------------------+-------------------+-------------------------+------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * join the dataframs to get both counts\n",
    "df_joined = df_bigram_from_trigram.join(\n",
    "    df_bigrams_count,\n",
    "    df_bigram_from_trigram[BIGRAM_FROM_TRIGRAM_COL] == df_bigrams_count[BIGRAM_COL],\n",
    "    \"left\",\n",
    ").select(\n",
    "    df_bigram_from_trigram[TRIGRAM_COL],\n",
    "    df_bigram_from_trigram[BIGRAM_FROM_TRIGRAM_COL],\n",
    "    df_bigram_from_trigram[BIGRAM_FROM_TRIGRAM_COUNT_COL],\n",
    "    df_bigrams_count[\"count\"].alias(BIGRAM_COUNT_COL),\n",
    ")\n",
    "show_dataframe(df_joined, prefix=\"Joined Dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Conditional Probability:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------------------+------------+-----------------------+\n",
      "|           trigram|bigram_from_trigram|bigram_from_trigram_count|bigram_count|condational_probability|\n",
      "+------------------+-------------------+-------------------------+------------+-----------------------+\n",
      "|      the covid 19|          the covid|                     1718|        1760|     0.9761363636363637|\n",
      "|            do n t|               do n|                     1662|        1662|                    1.0|\n",
      "|     the spread of|         the spread|                     1196|        1306|     0.9157733537519143|\n",
      "|       of covid 19|           of covid|                     1589|        1621|     0.9802590993214065|\n",
      "|        one of the|             one of|                      953|        1491|     0.6391683433936955|\n",
      "|           gt lt p|              gt lt|                     1023|        1792|     0.5708705357142857|\n",
      "|           p gt lt|               p gt|                     1037|        1936|     0.5356404958677686|\n",
      "|of the coronavirus|             of the|                      907|       17484|     0.0518760009151224|\n",
      "|           lt p gt|               lt p|                     1928|        1930|     0.9989637305699481|\n",
      "|     the number of|         the number|                     1037|        1103|     0.9401631912964642|\n",
      "+------------------+-------------------+-------------------------+------------+-----------------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * calculate the conditional probability\n",
    "df_result = df_joined.withColumn(\n",
    "    CONDITIONAL_PROBABILITY_COL, F.col(BIGRAM_FROM_TRIGRAM_COUNT_COL) / F.col(BIGRAM_COUNT_COL)\n",
    ")\n",
    "show_dataframe(df_result, prefix=\"Conditional Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Ranking over Partitions - in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME_Q3 = \"Question3\"\n",
    "BAKERY_CSV_FILE_PATH = \"./shared/data/Bakery.csv\"\n",
    "DATE_COL = \"Date\"\n",
    "TIME_COL = \"Time\"\n",
    "TRANSACTION_COL = \"Transaction\"\n",
    "ITEM_COL = \"Item\"\n",
    "BAKERY_CSV_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(DATE_COL, DateType(), False),\n",
    "        StructField(TIME_COL, StringType(), False),\n",
    "        StructField(TRANSACTION_COL, IntegerType(), False),\n",
    "        StructField(ITEM_COL, StringType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "DAYPART_COL = \"Daypart\"\n",
    "MORNING = \"morning\"\n",
    "NOON = \"noon\"\n",
    "AFTERNOON = \"afternoon\"\n",
    "EVENING = \"evening\"\n",
    "\n",
    "SALES_COL = \"Items Sold\"\n",
    "\n",
    "RANK_COL = \"Ranking\"\n",
    "\n",
    "TOP_THREE_ITEMS_COL = \"Top 3 Items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Bakery:\n",
      "\n",
      "+----------+--------+-----------+-------------+\n",
      "|      Date|    Time|Transaction|         Item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|2016-10-30|10:08:41|          4|       Muffin|\n",
      "|2016-10-30|10:13:03|          5|       Coffee|\n",
      "|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|2016-10-30|10:13:03|          5|        Bread|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME_Q3).master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df_bakery = spark.read.csv(BAKERY_CSV_FILE_PATH, header=True, schema=BAKERY_CSV_SCHEMA)\n",
    "show_dataframe(df_bakery, prefix=\"Bakery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Number of items sold by daypart:\n",
      "\n",
      "+---------+--------------------+----------+\n",
      "|  Daypart|                Item|Items Sold|\n",
      "+---------+--------------------+----------+\n",
      "|     noon|        Bare Popcorn|         1|\n",
      "|     noon|    My-5 Fruit Shoot|         7|\n",
      "|  morning|      Jammie Dodgers|        22|\n",
      "|     noon|    Christmas common|         5|\n",
      "|  evening|            Focaccia|         3|\n",
      "|  morning|          Chocolates|         2|\n",
      "|     noon|Drinking chocolat...|         2|\n",
      "|afternoon|           Empanadas|         3|\n",
      "|afternoon|Cherry me Dried f...|         1|\n",
      "|afternoon|                Cake|       480|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# * compute column Daypart based on time\n",
    "df_bakery = df_bakery.withColumn(\n",
    "    DAYPART_COL,\n",
    "    F.when((F.hour(TIME_COL) >= 6) & (F.hour(TIME_COL) < 11), MORNING)\n",
    "    .when((F.hour(TIME_COL) >= 11) & (F.hour(TIME_COL) < 14), NOON)\n",
    "    .when((F.hour(TIME_COL) >= 14) & (F.hour(TIME_COL) < 17), AFTERNOON)\n",
    "    .otherwise(EVENING),\n",
    ")\n",
    "\n",
    "# * count number of items sold per daypart\n",
    "df_daypart_sales = (\n",
    "    df_bakery.groupBy(DAYPART_COL, ITEM_COL).count().withColumnRenamed(\"count\", SALES_COL)\n",
    ")\n",
    "show_dataframe(df_daypart_sales, prefix=\"Number of items sold by daypart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Sales Ranking by Daypart:\n",
      "\n",
      "+---------+-------------+----------+-------+\n",
      "|  Daypart|         Item|Items Sold|Ranking|\n",
      "+---------+-------------+----------+-------+\n",
      "|afternoon|       Coffee|      1476|      1|\n",
      "|afternoon|        Bread|       847|      2|\n",
      "|afternoon|          Tea|       566|      3|\n",
      "|afternoon|         Cake|       480|      4|\n",
      "|afternoon|     Sandwich|       275|      5|\n",
      "|afternoon|Hot chocolate|       228|      6|\n",
      "|afternoon|      Cookies|       185|      7|\n",
      "|afternoon|    Alfajores|       167|      8|\n",
      "|afternoon|      Brownie|       150|      9|\n",
      "|afternoon|         NONE|       141|     10|\n",
      "+---------+-------------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "------------\n",
      "Top 3 Items Sold by Daypart:\n",
      "\n",
      "+---------+------+----------+-------+\n",
      "|  Daypart|  Item|Items Sold|Ranking|\n",
      "+---------+------+----------+-------+\n",
      "|afternoon|Coffee|      1476|      1|\n",
      "|afternoon| Bread|       847|      2|\n",
      "|afternoon|   Tea|       566|      3|\n",
      "|  evening|Coffee|        87|      1|\n",
      "|  evening| Bread|        55|      2|\n",
      "|  evening|   Tea|        49|      3|\n",
      "|  morning|Coffee|      1615|      1|\n",
      "|  morning| Bread|      1081|      2|\n",
      "|  morning|Pastry|       453|      3|\n",
      "|     noon|Coffee|      2293|      1|\n",
      "|     noon| Bread|      1342|      2|\n",
      "|     noon|   Tea|       540|      3|\n",
      "+---------+------+----------+-------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daypart_window = Window.partitionBy(DAYPART_COL).orderBy(F.col(SALES_COL).desc())\n",
    "\n",
    "df_ranked_daypart_sales = df_daypart_sales.withColumn(RANK_COL, F.rank().over(daypart_window))\n",
    "show_dataframe(df_ranked_daypart_sales, prefix=\"Sales Ranking by Daypart\")\n",
    "\n",
    "df_top_three_items = df_ranked_daypart_sales.filter(F.col(RANK_COL) <= 3)\n",
    "show_dataframe(df_top_three_items, prefix=\"Top 3 Items Sold by Daypart\", num_rows_to_show=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Top 3 Items Sold by Daypart Aggregated:\n",
      "\n",
      "+---------+--------------------+\n",
      "|  Daypart|         Top 3 Items|\n",
      "+---------+--------------------+\n",
      "|afternoon|  Coffee, Bread, Tea|\n",
      "|  evening|  Coffee, Bread, Tea|\n",
      "|  morning|Coffee, Bread, Pa...|\n",
      "|     noon|  Coffee, Bread, Tea|\n",
      "+---------+--------------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result = df_top_three_items.groupBy(DAYPART_COL).agg(\n",
    "    F.concat_ws(\", \", F.collect_list(F.col(ITEM_COL))).alias(TOP_THREE_ITEMS_COL)\n",
    ")\n",
    "show_dataframe(df_result, prefix=\"Top 3 Items Sold by Daypart Aggregated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Duplicate Detection with Minhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "\n",
    "Shout out to [this explanation](https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/) on Minhash/LSH (Local Sensitive Hashing) Algorithm, which greatly helped me understand the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME_Q4 = \"Question4\"\n",
    "HUFFPOST_JSON_FILE_PATH = \"./shared/data/Huffpost.json\"\n",
    "BASE_NEWS_ITEM_SHORT_DESCRIPTION = (\n",
    "    \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She\\u2019s Perfect\"\n",
    ")\n",
    "\n",
    "SHORT_DESC_COL = \"short_description\"\n",
    "TOKENS_COL = \"tokens\"\n",
    "VECTOR_COL = \"vectors\"\n",
    "\n",
    "MINHASH_COL = \"minhash\"\n",
    "NUM_HASHTABLES = 5\n",
    "\n",
    "JACCARD_SIM_COL = \"jaccard_similarity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Huffpost:\n",
      "\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+\n",
      "|             authors|      category|      date|            headline|                link|   short_description|\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+\n",
      "|Carla K. Johnson, AP|     U.S. NEWS|2022-09-23|Over 4 Million Am...|https://www.huffp...|Health experts sa...|\n",
      "|      Mary Papenfuss|     U.S. NEWS|2022-09-23|American Airlines...|https://www.huffp...|He was subdued by...|\n",
      "|       Elyse Wanshel|        COMEDY|2022-09-23|23 Of The Funnies...|https://www.huffp...|\"Until you have a...|\n",
      "|    Caroline Bologna|     PARENTING|2022-09-23|The Funniest Twee...|https://www.huffp...|\"Accidentally put...|\n",
      "|      Nina Golgowski|     U.S. NEWS|2022-09-22|Woman Who Called ...|https://www.huffp...|Amy Cooper accuse...|\n",
      "|                    |     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|https://www.huffp...|The 63-year-old w...|\n",
      "|       Elyse Wanshel|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|https://www.huffp...|\"Who's that behin...|\n",
      "|     DÁNICA COTO, AP|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|https://www.huffp...|More than half a ...|\n",
      "|         Marina Fang|CULTURE & ARTS|2022-09-22|How A New Documen...|https://www.huffp...|In \"Mija,\" direct...|\n",
      "|   Aamer Madhani, AP|    WORLD NEWS|2022-09-21|Biden At UN To Ca...|https://www.huffp...|White House offic...|\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME_Q4).master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df_huffpost = spark.read.json(HUFFPOST_JSON_FILE_PATH)\n",
    "show_dataframe(df_huffpost, prefix=\"Huffpost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Vectorized Huffposts:\n",
      "\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             authors|      category|      date|            headline|                link|   short_description|              tokens|             vectors|\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Carla K. Johnson, AP|     U.S. NEWS|2022-09-23|Over 4 Million Am...|https://www.huffp...|Health experts sa...|[health, experts,...|(191855,[0,1,3,6,...|\n",
      "|      Mary Papenfuss|     U.S. NEWS|2022-09-23|American Airlines...|https://www.huffp...|He was subdued by...|[he, was, subdued...|(191855,[0,1,3,4,...|\n",
      "|       Elyse Wanshel|        COMEDY|2022-09-23|23 Of The Funnies...|https://www.huffp...|\"Until you have a...|[\"until, you, hav...|(191855,[2,11,15,...|\n",
      "|    Caroline Bologna|     PARENTING|2022-09-23|The Funniest Twee...|https://www.huffp...|\"Accidentally put...|[\"accidentally, p...|(191855,[2,4,5,9,...|\n",
      "|      Nina Golgowski|     U.S. NEWS|2022-09-22|Woman Who Called ...|https://www.huffp...|Amy Cooper accuse...|[amy, cooper, acc...|(191855,[0,2,3,4,...|\n",
      "|                    |     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|https://www.huffp...|The 63-year-old w...|[the, 63-year-old...|(191855,[0,10,20,...|\n",
      "|       Elyse Wanshel|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|https://www.huffp...|\"Who's that behin...|[\"who's, that, be...|(191855,[7,8,17,2...|\n",
      "|     DÁNICA COTO, AP|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|https://www.huffp...|More than half a ...|[more, than, half...|(191855,[0,2,34,5...|\n",
      "|         Marina Fang|CULTURE & ARTS|2022-09-22|How A New Documen...|https://www.huffp...|In \"Mija,\" direct...|[in, \"mija,\", dir...|(191855,[0,1,2,3,...|\n",
      "|   Aamer Madhani, AP|    WORLD NEWS|2022-09-21|Biden At UN To Ca...|https://www.huffp...|White House offic...|[white, house, of...|(191855,[0,1,2,3,...|\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "['!!', '!!!', '!!!!\"', '!t', '\"', '\"\"incapable\"', '\"#aspenideas\"', '\"#baldwinitoseverywhere,\"', '\"#bars', '\"#beautiful\"']\n",
      "------------\n",
      "Vectorized Base Description:\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|   short_description|              tokens|             vectors|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Kitten Born With ...|[kitten, born, wi...|(191855,[2,4,12,4...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=SHORT_DESC_COL, outputCol=TOKENS_COL, pattern=\"\\\\s+\", gaps=True)\n",
    "vectorizer = CountVectorizer(inputCol=TOKENS_COL, outputCol=VECTOR_COL)\n",
    "\n",
    "# Build the pipeline\n",
    "df_huffpost_tokenized = tokenizer.transform(df_huffpost).filter(F.size(F.col(TOKENS_COL)) > 0)\n",
    "model_vectorizer = vectorizer.fit(df_huffpost_tokenized)\n",
    "df_vectorized = model_vectorizer.transform(df_huffpost_tokenized)\n",
    "show_dataframe(df_vectorized, prefix=\"Vectorized Huffposts\")\n",
    "print(sorted(model_vectorizer.vocabulary)[:10])\n",
    "\n",
    "df_base_desc = spark.createDataFrame([(BASE_NEWS_ITEM_SHORT_DESCRIPTION,)], [SHORT_DESC_COL])\n",
    "df_base_desc_tokenized = tokenizer.transform(df_base_desc)\n",
    "df_base_desc_vectorized = model_vectorizer.transform(df_base_desc_tokenized)\n",
    "show_dataframe(df_base_desc_vectorized, prefix=\"Vectorized Base Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Minhashed Huffpost:\n",
      "\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             authors|      category|      date|            headline|                link|   short_description|              tokens|             vectors|             minhash|\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Carla K. Johnson, AP|     U.S. NEWS|2022-09-23|Over 4 Million Am...|https://www.huffp...|Health experts sa...|[health, experts,...|(191855,[0,1,3,6,...|[[1.7908219E7], [...|\n",
      "|      Mary Papenfuss|     U.S. NEWS|2022-09-23|American Airlines...|https://www.huffp...|He was subdued by...|[he, was, subdued...|(191855,[0,1,3,4,...|[[1.7908219E7], [...|\n",
      "|       Elyse Wanshel|        COMEDY|2022-09-23|23 Of The Funnies...|https://www.huffp...|\"Until you have a...|[\"until, you, hav...|(191855,[2,11,15,...|[[1.00906059E8], ...|\n",
      "|    Caroline Bologna|     PARENTING|2022-09-23|The Funniest Twee...|https://www.huffp...|\"Accidentally put...|[\"accidentally, p...|(191855,[2,4,5,9,...|[[1.38996082E8], ...|\n",
      "|      Nina Golgowski|     U.S. NEWS|2022-09-22|Woman Who Called ...|https://www.huffp...|Amy Cooper accuse...|[amy, cooper, acc...|(191855,[0,2,3,4,...|[[1.7908219E7], [...|\n",
      "|                    |     U.S. NEWS|2022-09-22|Cleaner Was Dead ...|https://www.huffp...|The 63-year-old w...|[the, 63-year-old...|(191855,[0,10,20,...|[[7.9236213E7], [...|\n",
      "|       Elyse Wanshel|     U.S. NEWS|2022-09-22|Reporter Gets Ado...|https://www.huffp...|\"Who's that behin...|[\"who's, that, be...|(191855,[7,8,17,2...|[[6.5468779E7], [...|\n",
      "|     DÁNICA COTO, AP|    WORLD NEWS|2022-09-22|Puerto Ricans Des...|https://www.huffp...|More than half a ...|[more, than, half...|(191855,[0,2,34,5...|[[1.09340674E8], ...|\n",
      "|         Marina Fang|CULTURE & ARTS|2022-09-22|How A New Documen...|https://www.huffp...|In \"Mija,\" direct...|[in, \"mija,\", dir...|(191855,[0,1,2,3,...|[[1.7908219E7], [...|\n",
      "|   Aamer Madhani, AP|    WORLD NEWS|2022-09-21|Biden At UN To Ca...|https://www.huffp...|White House offic...|[white, house, of...|(191855,[0,1,2,3,...|[[1.7908219E7], [...|\n",
      "+--------------------+--------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "------------\n",
      "\n",
      "------------\n",
      "Minhashed Base Description:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|   short_description|              tokens|             vectors|             minhash|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Kitten Born With ...|[kitten, born, wi...|(191855,[2,4,12,4...|[[2.63406707E8], ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minhash_lsh = MinHashLSH(inputCol=VECTOR_COL, outputCol=MINHASH_COL, numHashTables=NUM_HASHTABLES)\n",
    "model_minhash = minhash_lsh.fit(df_vectorized)\n",
    "df_minhashed = model_minhash.transform(df_vectorized)\n",
    "show_dataframe(df_minhashed, prefix=\"Minhashed Huffpost\")\n",
    "\n",
    "df_base_minhashed = model_minhash.transform(df_base_desc_vectorized)\n",
    "show_dataframe(df_base_minhashed, prefix=\"Minhashed Base Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Top 5 Similar Items (Duplicate Removed):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 318:>                                                      (0 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|   short_description|   short_description|jaccard_similarity|\n",
      "+--------------------+--------------------+------------------+\n",
      "|Kitten Born With ...|A cookie with the...| 0.782608695652174|\n",
      "|Kitten Born With ...|With a back flip ...|0.8235294117647058|\n",
      "|Kitten Born With ...|The stories of an...|0.8260869565217391|\n",
      "|Kitten Born With ...|Who needs a scrip...|0.8333333333333334|\n",
      "|Kitten Born With ...|Traveling as a te...|0.8461538461538461|\n",
      "+--------------------+--------------------+------------------+\n",
      "\n",
      "------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = model_minhash.approxSimilarityJoin(\n",
    "    df_base_minhashed, df_minhashed, float(\"inf\"), distCol=JACCARD_SIM_COL\n",
    ")\n",
    "\n",
    "df_top_five = df_result.select(\n",
    "    f\"datasetA.{SHORT_DESC_COL}\", f\"datasetB.{SHORT_DESC_COL}\", JACCARD_SIM_COL\n",
    ").orderBy(JACCARD_SIM_COL).filter(\n",
    "    f\"datasetA.{SHORT_DESC_COL} != datasetB.{SHORT_DESC_COL}\"\n",
    ").limit(5)\n",
    "\n",
    "show_dataframe(df_top_five, prefix=\"Top 5 Similar Items (Duplicate Removed)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
