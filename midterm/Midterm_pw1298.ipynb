{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9ddd8f-06ae-4cf7-a585-66ddc429b81c",
   "metadata": {},
   "source": [
    "### 2. Spark – Language Models - in Spark - 50 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d645e1-dd3f-4663-8238-8154fd8ed9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, lower, regexp_replace, split, sum, hour, when, count, rank, concat_ws, struct, collect_list\n",
    "from pyspark.ml.feature import Tokenizer, NGram\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71851f89-d45d-496d-87af-6570ecb3ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/04 13:08:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Language Models with Trigrams\").getOrCreate()\n",
    "\n",
    "# Load the text data\n",
    "text_files = [\"data/20-01.txt\", \"data/20-02.txt\", \"data/20-03.txt\", \"data/20-04.txt\", \"data/20-05.txt\"]\n",
    "df_text = spark.read.text(text_files)\n",
    "\n",
    "# Prepare the text data\n",
    "df_cleaned = df_text.select(lower(regexp_replace(col(\"value\"), \"[^0-9a-zA-Z]+\", \" \")).alias(\"text\"))\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706c9cd4-721a-438c-9c07-7388e8e387a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df_words = tokenizer.transform(df_cleaned)\n",
    "\n",
    "# Generate trigrams\n",
    "ngram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
    "df_trigrams = ngram.transform(df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52dbd068-6501-4384-a15d-7b85526a25fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/04 13:08:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|trigram           |count|\n",
      "+------------------+-----+\n",
      "|lt p gt           |1928 |\n",
      "|the covid 19      |1718 |\n",
      "|do n t            |1662 |\n",
      "|of covid 19       |1589 |\n",
      "|the spread of     |1196 |\n",
      "|p gt lt           |1037 |\n",
      "|the number of     |1037 |\n",
      "|gt lt p           |1023 |\n",
      "|one of the        |953  |\n",
      "|of the coronavirus|907  |\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode trigrams into separate rows and count occurrences\n",
    "df_trigram_counts = df_trigrams.select(explode(col(\"trigrams\")).alias(\"trigram\")).groupBy(\"trigram\").count()\n",
    "\n",
    "# Get the top 10 trigrams\n",
    "top_trigrams = df_trigram_counts.orderBy(col(\"count\").desc()).limit(10)\n",
    "top_trigrams.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1659870a-f838-4443-9eca-bd88186c96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|trigram           |probability|\n",
      "+------------------+-----------+\n",
      "|do n t            |1.0        |\n",
      "|gt lt p           |1.0        |\n",
      "|lt p gt           |1.0        |\n",
      "|of covid 19       |1.0        |\n",
      "|of the coronavirus|1.0        |\n",
      "|one of the        |1.0        |\n",
      "|p gt lt           |1.0        |\n",
      "|the covid 19      |1.0        |\n",
      "|the number of     |1.0        |\n",
      "|the spread of     |1.0        |\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split trigram into individual words\n",
    "df_split_trigrams = top_trigrams.withColumn(\"words\", split(col(\"trigram\"), \" \"))\n",
    "\n",
    "# Create a window specification\n",
    "windowSpec = Window.partitionBy(col(\"words\")[0], col(\"words\")[1])\n",
    "\n",
    "# Compute conditional probabilities\n",
    "df_probabilities = df_split_trigrams.withColumn(\"probability\", col(\"count\") / sum(\"count\").over(windowSpec))\n",
    "\n",
    "df_probabilities.select(col(\"trigram\"), col(\"probability\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5c21c-328d-4239-b8bc-c99035a1bc85",
   "metadata": {},
   "source": [
    "### 3. Ranking over Partitions – in Spark. - 50 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b4b72c-5339-4047-bb1c-97f9d8ce20c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/04 13:09:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------+-------------+\n",
      "|      Date|               Time|Transaction|         Item|\n",
      "+----------+-------------------+-----------+-------------+\n",
      "|2016-10-30|2024-11-04 09:58:11|          1|        Bread|\n",
      "|2016-10-30|2024-11-04 10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|2024-11-04 10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|2024-11-04 10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|2024-11-04 10:07:57|          3|          Jam|\n",
      "+----------+-------------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Daypart Item Ranking\").getOrCreate()\n",
    "\n",
    "df_bakery = spark.read.csv('shared/data/Bakery.csv', header=True, inferSchema=True)\n",
    "df_bakery.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af06742-11c5-41f5-9ca2-d336a6d7ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+\n",
      "|Daypart|            Item|ItemCount|\n",
      "+-------+----------------+---------+\n",
      "|   noon|    Bare Popcorn|        1|\n",
      "|   noon|My-5 Fruit Shoot|        7|\n",
      "|morning|  Jammie Dodgers|       22|\n",
      "|   noon|Christmas common|        5|\n",
      "|evening|        Focaccia|        3|\n",
      "+-------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_bakery = df_bakery.withColumn(\n",
    "    \"Daypart\",\n",
    "    when((hour(\"Time\") >= 6) & (hour(\"Time\") < 11), \"morning\")\n",
    "    .when((hour(\"Time\") >= 11) & (hour(\"Time\") < 14), \"noon\")\n",
    "    .when((hour(\"Time\") >= 14) & (hour(\"Time\") < 17), \"afternoon\")\n",
    "    .otherwise(\"evening\")\n",
    ")\n",
    "\n",
    "# Count Items Sold Per Daypart\n",
    "df_daypart_items = df_bakery.groupBy(\"Daypart\", \"Item\").agg(count(\"Item\").alias(\"ItemCount\"))\n",
    "df_daypart_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b84482f-07b7-4e28-a75d-8e296dc26eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+----+\n",
      "|  Daypart|    Item|ItemCount|Rank|\n",
      "+---------+--------+---------+----+\n",
      "|afternoon|  Coffee|     1476|   1|\n",
      "|afternoon|   Bread|      847|   2|\n",
      "|afternoon|     Tea|      566|   3|\n",
      "|afternoon|    Cake|      480|   4|\n",
      "|afternoon|Sandwich|      275|   5|\n",
      "+---------+--------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"Daypart\").orderBy(col(\"ItemCount\").desc())\n",
    "# Rank Items Within Each Daypart\n",
    "df_ranked_items = df_daypart_items.withColumn(\"Rank\", rank().over(windowSpec))\n",
    "df_ranked_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0f891f-f12a-4c5c-9acf-73476c7f1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n",
      "|Daypart  |TopItems             |\n",
      "+---------+---------------------+\n",
      "|afternoon|Coffee, Bread, Tea   |\n",
      "|evening  |Coffee, Bread, Tea   |\n",
      "|morning  |Coffee, Bread, Pastry|\n",
      "|noon     |Coffee, Bread, Tea   |\n",
      "+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for Top 3 Items\n",
    "df_top_items = df_ranked_items.filter(col(\"Rank\") <= 3)\n",
    "\n",
    "# Format the Output using concat_ws\n",
    "df_final_output = df_top_items.groupBy(\"Daypart\").agg(concat_ws(\", \", collect_list(col(\"Item\"))).alias(\"TopItems\"))\n",
    "\n",
    "df_final_output.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca2f98-4037-4482-8aad-ea4ddb2fb536",
   "metadata": {},
   "source": [
    "### 4. Duplicate Detection with Minhash – 50 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92acc075-415b-427d-82d1-615e1d45a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/04 13:09:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- headline: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- short_description: string (nullable = true)\n",
      "\n",
      "+--------------------+---------+----------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|authors             |category |date      |headline                                                                                 |link                                                                                                                                  |short_description                                                                                                                                              |\n",
      "+--------------------+---------+----------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Carla K. Johnson, AP|U.S. NEWS|2022-09-23|Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters             |https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9                                                    |Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.     |\n",
      "|Mary Papenfuss      |U.S. NEWS|2022-09-23|American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video|https://www.huffpost.com/entry/american-airlines-passenger-banned-flight-attendant-punch-justice-department_n_632e25d3e4b0e247890329fe|He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.|\n",
      "|Elyse Wanshel       |COMEDY   |2022-09-23|23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)                    |https://www.huffpost.com/entry/funniest-tweets-cats-dogs-september-17-23_n_632de332e4b0695c1d81dc02                                   |\"Until you have a dog you don't understand what could be eaten.\"                                                                                               |\n",
      "|Caroline Bologna    |PARENTING|2022-09-23|The Funniest Tweets From Parents This Week (Sept. 17-23)                                 |https://www.huffpost.com/entry/funniest-parenting-tweets_l_632d7d15e4b0d12b5403e479                                                   |\"Accidentally put grown-up toothpaste on my toddler’s toothbrush and he screamed like I was cleaning his teeth with a Carolina Reaper dipped in Tabasco sauce.\"|\n",
      "|Nina Golgowski      |U.S. NEWS|2022-09-22|Woman Who Called Cops On Black Bird-Watcher Loses Lawsuit Against Ex-Employer            |https://www.huffpost.com/entry/amy-cooper-loses-discrimination-lawsuit-franklin-templeton_n_632c6463e4b09d8701bd227e                  |Amy Cooper accused investment firm Franklin Templeton of unfairly firing her and branding her a racist after video of the Central Park encounter went viral.   |\n",
      "+--------------------+---------+----------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, MinHashLSH, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinHash LSH for Similarity Detection\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"shared/data/Huffpost.json\")\n",
    "\n",
    "# Show the data structure\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02cba89-8968-4ade-b169-74450a06bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"short_description\", outputCol=\"tokens\")\n",
    "vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer])\n",
    "model = pipeline.fit(df)\n",
    "df_transformed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c11bb1a-11cb-49de-ac5d-43433c2022da",
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "model_lsh = minhash.fit(df_transformed)\n",
    "df_lsh = model_lsh.transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dff47f1-f990-4a9f-8356-1d157fa53ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/04 13:10:13 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/11/04 13:10:14 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/11/04 13:11:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/11/04 13:11:11 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "[Stage 47:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+------------------+\n",
      "|short_description                                                         |short_description                                                                            |JaccardDistance   |\n",
      "+--------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+------------------+\n",
      "|Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect|”Maybe she’s born with it ... Maybe she’s a tired mom who doesn’t have time for this.”       |0.75              |\n",
      "|Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect|With a back flip and everything.                                                             |0.8235294117647058|\n",
      "|Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect|Who needs a script and budget anyway?                                                        |0.8333333333333334|\n",
      "|Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect|She’s back, b***hes, with a brand new album.                                                 |0.8421052631578947|\n",
      "|Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect|\"They do a little part on television and everyone knows who they are. They can't really act.\"|0.8461538461538461|\n",
      "+--------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the feature vector of the base item\n",
    "base_description = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She\\u2019s Perfect\"\n",
    "base_df = spark.createDataFrame([(base_description,)], [\"short_description\"])\n",
    "base_transformed = model.transform(base_df)\n",
    "\n",
    "# Join the base item with the dataset to find similarities\n",
    "base_hashed = model_lsh.transform(base_transformed)\n",
    "join_results = model_lsh.approxSimilarityJoin(base_hashed, df_lsh, float(\"inf\"), distCol=\"JaccardDistance\")\n",
    "\n",
    "# Select the top 5 most similar item\n",
    "join_results.select(\"datasetA.short_description\", \"datasetB.short_description\", \"JaccardDistance\").orderBy(\"JaccardDistance\") \\\n",
    "    .filter(\"datasetA.short_description != datasetB.short_description\") \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9ea01-c22f-4aac-8742-8b61082d75bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
