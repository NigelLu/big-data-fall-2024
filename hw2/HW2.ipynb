{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-GY 6513 Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, ArrayType, FloatType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAKERY_CSV_FILE_PATH = \"./Bakery.csv\"\n",
    "BAKERY_CSV_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), False),\n",
    "        StructField(\"Time\", StringType(), False),\n",
    "        StructField(\"Transaction\", IntegerType(), False),\n",
    "        StructField(\"Item\", StringType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "RESTAURANT_IN_NC_JSON_FILE_PATH = \"./Restaurants_in_Durham_County_NC (2).json\"\n",
    "RESTAURANT_IN_NC_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"datasetid\", StringType(), False),\n",
    "        StructField(\"recordid\", StringType(), False),\n",
    "        StructField(\n",
    "            \"fields\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"status\", StringType(), False),\n",
    "                    StructField(\"geolocation\", ArrayType(FloatType()), False),\n",
    "                    StructField(\"premise_zip\", StringType(), False),\n",
    "                    StructField(\"rpt_area_desc\", StringType(), False),\n",
    "                    StructField(\"risk\", IntegerType(), False),\n",
    "                    StructField(\"est_group_desc\", StringType(), False),\n",
    "                    StructField(\"seats\", IntegerType(), False),\n",
    "                    StructField(\"water\", StringType(), False),\n",
    "                    StructField(\"premise_phone\", StringType(), False),\n",
    "                    StructField(\"premise_state\", StringType(), False),\n",
    "                    StructField(\"insp_freq\", IntegerType(), False),\n",
    "                    StructField(\"type_description\", StringType(), False),\n",
    "                    StructField(\"premise_city\", StringType(), False),\n",
    "                    StructField(\"premise_address2\", StringType(), False),\n",
    "                    StructField(\"opening_date\", TimestampType(), False),\n",
    "                    StructField(\"premise_name\", StringType(), False),\n",
    "                    StructField(\"transitional_type_desc\", StringType(), False),\n",
    "                    StructField(\"smoking_allowed\", StringType(), False),\n",
    "                    StructField(\"id\", StringType(), False),\n",
    "                    StructField(\"sewage\", StringType(), False),\n",
    "                    StructField(\"premise_address1\", StringType(), False),\n",
    "                ]\n",
    "            ),\n",
    "            False,\n",
    "        ),\n",
    "        StructField(\n",
    "            \"geometry\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"type\", StringType(), False),\n",
    "                    StructField(\"coordinates\", ArrayType(FloatType()), False),\n",
    "                ]\n",
    "            ),\n",
    "            False,\n",
    "        ),\n",
    "        StructField(\"record_timestamp\", TimestampType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "POPULATION_BY_COUNTRY_CSV_FILE_PATH = \"./populationbycountry19802010millions (1).csv\"\n",
    "POPULATION_BY_COUNTRY_SCHEMA = StructType(\n",
    "    [StructField(\"Country\", StringType(), False)]\n",
    "    + [StructField(f\"{year}\", FloatType(), False) for year in range(1980, 2011)]\n",
    ")\n",
    "\n",
    "WORD_TEXT_FILE_PATH = \"./hw1text/*.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"BigDataHomework2\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n",
      "|      Date|    Time|Transaction|         Item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|      NULL|    Time|       NULL|         Item|\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bakery = spark.read.csv(BAKERY_CSV_FILE_PATH, header=False, schema=BAKERY_CSV_SCHEMA)\n",
    "df_bakery.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|       datasetid|            recordid|              fields|            geometry|   record_timestamp|\n",
      "+----------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|restaurants-data|1644654b953d1802c...|{ACTIVE, [35.9207...|{Point, [-78.9573...|2017-07-13 09:15:31|\n",
      "|restaurants-data|93573dbf8c9e799d8...|{ACTIVE, [36.0467...|{Point, [-78.8895...|2017-07-13 09:15:31|\n",
      "|restaurants-data|0d274200c7cef50d0...|{ACTIVE, [35.9182...|{Point, [-78.9593...|2017-07-13 09:15:31|\n",
      "|restaurants-data|cf3e0b175a6ebad2a...|{ACTIVE, [36.0183...|{Point, [-78.9060...|2017-07-13 09:15:31|\n",
      "|restaurants-data|e796570677f7c39cc...|{ACTIVE, [36.0556...|{Point, [-78.9135...|2017-07-13 09:15:31|\n",
      "+----------------+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_restaurant_in_nc = spark.read.json(RESTAURANT_IN_NC_JSON_FILE_PATH, schema=RESTAURANT_IN_NC_SCHEMA)\n",
    "df_restaurant_in_nc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+--------+---------+---------+---------+--------+---------+--------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|      Country|     1980|     1981|     1982|     1983|     1984|    1985|     1986|     1987|    1988|     1989|    1990|     1991|     1992|     1993|    1994|     1995|    1996|     1997|     1998|     1999|    2000|     2001|     2002|     2003|     2004|     2005|     2006|     2007|     2008|     2009|     2010|\n",
      "+-------------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+--------+---------+---------+---------+--------+---------+--------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|North America|320.27637|324.44693|328.62015|332.72488|336.72144|340.7481|344.89548|349.07828|353.2939|357.68457|362.4468|367.70685|373.29068|378.74234|383.9166|388.97217|393.9428|398.97205|403.85583|408.60297|413.3245|417.83237|422.05267|426.06238|430.26938|434.47232|438.82965| 443.3473|447.67395|451.83698|456.59332|\n",
      "|      Bermuda|  0.05473|  0.05491|  0.05517|  0.05551|  0.05585| 0.05618|  0.05651|  0.05683| 0.05717|  0.05749| 0.05778|   0.0581|   0.0587|  0.05924| 0.05975|  0.06029| 0.06087|  0.06145|  0.06198|  0.06251| 0.06306|  0.06361|  0.06418|  0.06476|  0.06534|  0.06591|  0.06644|  0.06692|  0.06739|  0.06784|  0.06827|\n",
      "|       Canada|  24.5933|     24.9|  25.2019|  25.4563|  25.7018| 25.9416|  26.2038|  26.5497| 26.8948|  27.3793| 27.7906|  28.1179| 28.54489| 28.95334|29.33081| 29.69053|30.02632|  30.3056| 30.55166| 30.82026|31.09956| 31.37674| 31.64096| 31.88931| 32.13476| 32.38638| 32.65668| 32.93596|  33.2127| 33.48721| 33.75974|\n",
      "|    Greenland|  0.05021|  0.05103|  0.05166|  0.05211|  0.05263| 0.05315|  0.05364|   0.0541| 0.05485|  0.05541| 0.05563|  0.05554|  0.05549|  0.05564| 0.05592|  0.05619| 0.05634|  0.05651|  0.05661|   0.0567| 0.05689|  0.05713|  0.05736|  0.05754|   0.0577|  0.05778|  0.05764|  0.05753|  0.05756|   0.0576|  0.05764|\n",
      "|       Mexico| 68.34748| 69.96926|  71.6409| 73.36288| 75.08014|76.76723| 78.44243| 80.12249|81.78182| 83.36684|84.91365| 86.48803| 88.11103| 89.74914| 91.3379| 92.88035|94.39858| 95.89515| 97.32506| 98.61691|99.92662|101.24696|102.47993|103.71806|104.95959| 106.2029|107.44953|108.70089| 109.9554|111.21179|112.46886|\n",
      "+-------------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+--------+---------+---------+---------+--------+---------+--------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_population_by_country = spark.read.csv(POPULATION_BY_COUNTRY_CSV_FILE_PATH)\n",
    "# * remove the first row\n",
    "row_count = df_population_by_country.count()\n",
    "df_population_by_country = df_population_by_country.limit(row_count - 1).subtract(\n",
    "    df_population_by_country.limit(1)\n",
    ")\n",
    "\n",
    "# * manually add column names\n",
    "# * since the first column name is missing\n",
    "idx = 0\n",
    "for field in POPULATION_BY_COUNTRY_SCHEMA:\n",
    "    df_population_by_country = df_population_by_country.withColumnRenamed(\n",
    "        f\"_c{idx}\", field.name\n",
    "    ).withColumn(field.name, F.col(field.name).cast(field.dataType))\n",
    "    idx += 1\n",
    "df_population_by_country.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               value|        cleaned text|               words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|@@31678741 <p> If...| 31678741 p f pro...|[31678741, p, f, ...|\n",
      "|@@31680641 <p> In...| 31680641 p n the...|[31680641, p, n, ...|\n",
      "|@@31680841 <p> Th...| 31680841 p housa...|[31680841, p, hou...|\n",
      "|@@31682241 <h> Le...| 31682241 h esson...|[31682241, h, ess...|\n",
      "|@@31683241 <h> OE...| 31683241 h warns...|[31683241, h, war...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text = spark.read.text(WORD_TEXT_FILE_PATH)\n",
    "\n",
    "# * pre-process data\n",
    "# * lower case the words and replace all characters not [0-9a-z] with spaces\n",
    "df_text_cleaned = df_text.withColumn(\n",
    "    \"cleaned text\", F.lower(F.regexp_replace(F.col(\"value\"), \"[^0-9a-z]+\", \" \"))\n",
    ")\n",
    "# * split the text into tokens separated by \"s+\"\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"cleaned text\", outputCol=\"words\", pattern=\"\\\\s+\")\n",
    "df_text_tokenized = regex_tokenizer.transform(df_text_cleaned)\n",
    "df_text_tokenized = df_text_tokenized.filter(\n",
    "    F.col(\"value\").isNotNull() & F.col(\"cleaned text\").isNotNull()\n",
    ")\n",
    "df_text_tokenized = df_text_tokenized.filter(\n",
    "    (df_text_tokenized[\"value\"] != \"\") & (df_text_tokenized[\"cleaned text\"] != \"\")\n",
    ")\n",
    "\n",
    "df_text_tokenized.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 1 Selling Item on Monday from 7AM - 11AM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---+\n",
      "|Hour|  Item|qty|\n",
      "+----+------+---+\n",
      "|  10|Coffee|114|\n",
      "+----+------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_weekday = \"Monday\"\n",
    "target_time_range = [7, 11]\n",
    "df_bakery_q1 = (\n",
    "    (\n",
    "        df_bakery.withColumn(\"Weekday\", F.date_format(F.col(\"Date\"), \"EEEE\"))\n",
    "        .withColumn(\"Hour\", F.hour(F.col(\"Time\")))\n",
    "        .filter(\n",
    "            (F.col(\"Weekday\") == target_weekday)\n",
    "            & (F.col(\"Hour\").between(target_time_range[0], target_time_range[1]))\n",
    "        )\n",
    "    )\n",
    "    .groupBy(\"Hour\", \"Item\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"qty\")\n",
    ")\n",
    "df_bakery_q1.show(1) # * only show the item with highest selling count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 2 (by qty) Items Bought by Daypart and DayType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------------+\n",
      "|Daypart  |DayType|Top_2_Items    |\n",
      "+---------+-------+---------------+\n",
      "|Breakfast|Weekday|[Coffee, Bread]|\n",
      "|Breakfast|Weekend|[Coffee, Bread]|\n",
      "|Dinner   |Weekday|[Coffee, Bread]|\n",
      "|Dinner   |Weekend|[Coffee, Bread]|\n",
      "|Lunch    |Weekday|[Coffee, Bread]|\n",
      "|Lunch    |Weekend|[Coffee, Bread]|\n",
      "+---------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_col = F.col(\"Time\")\n",
    "hour_col = F.col(\"Hour\")\n",
    "date_col = F.col(\"Date\")\n",
    "day_of_week_col = F.col(\"DayofWeek\")\n",
    "\n",
    "df_bakery_q2 = (\n",
    "    df_bakery.withColumn(\"Hour\", F.hour(time_col))\n",
    "    .withColumn(\n",
    "        \"Daypart\",\n",
    "        F.when((hour_col >= 6) & (hour_col <= 10), \"Breakfast\")\n",
    "        .when((hour_col >= 11) & (hour_col <= 15), \"Lunch\")\n",
    "        .otherwise(\"Dinner\"),\n",
    "    )\n",
    "    .withColumn(\"DayofWeek\", F.date_format(date_col, \"EEEE\"))\n",
    "    .withColumn(\n",
    "        \"DayType\",\n",
    "        F.when((day_of_week_col == \"Saturday\") | (day_of_week_col == \"Sunday\"), \"Weekend\").otherwise(\n",
    "            \"Weekday\"\n",
    "        ),\n",
    "    )\n",
    ")  # * compute all necessary columns\n",
    "\n",
    "df_bakery_q2 = df_bakery_q2.groupBy(\"Daypart\", \"DayType\", \"Item\").count()\n",
    "\n",
    "window = Window.partitionBy(\"Daypart\", \"DayType\").orderBy(F.desc(\"count\"))\n",
    "df_bakery_q2 = (\n",
    "    (\n",
    "        df_bakery_q2.withColumn(\"ranking\", F.row_number().over(window))\n",
    "        .filter(F.col(\"ranking\") <= 2)\n",
    "        .groupBy(\"Daypart\", \"DayType\")\n",
    "        .agg(F.collect_list(\"Item\").alias(\"Top_2_Items\"))\n",
    "    )\n",
    "    .select(\"Daypart\", \"DayType\", \"Top_2_Items\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the Number of Entities by “fields.rpt_area_desc”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|rpt_area_desc        |count|\n",
      "+---------------------+-----+\n",
      "|Bed&Breakfast Home   |4    |\n",
      "|Summer Camps         |4    |\n",
      "|Institutions         |30   |\n",
      "|Local Confinement    |2    |\n",
      "|Mobile Food          |147  |\n",
      "|School Buildings     |89   |\n",
      "|Summer Food          |242  |\n",
      "|Swimming Pools       |420  |\n",
      "|Day Care             |173  |\n",
      "|Tattoo Establishments|36   |\n",
      "|Residential Care     |154  |\n",
      "|Bed&Breakfast Inn    |2    |\n",
      "|Adult Day Care       |5    |\n",
      "|Lodging              |62   |\n",
      "|Food Service         |1093 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_restaurant_in_nc_q3 = df_restaurant_in_nc.groupBy(F.col(\"fields.rpt_area_desc\")).count()\n",
    "df_restaurant_in_nc_q3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Biggest Percentage Increase from 1990 to 2000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country with the biggest percentage increase:\n",
      "+--------------------+-----------------+\n",
      "|             Country|Percentage Change|\n",
      "+--------------------+-----------------+\n",
      "|United Arab Emirates|76.27926665641841|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "Country with the biggest percentage decrease:\n",
      "+----------+-----------------+\n",
      "|   Country|Percentage Change|\n",
      "+----------+-----------------+\n",
      "|Montserrat|-63.1873277639145|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# * Select only the necessary columns for the calculation\n",
    "df_population = df_population_by_country.filter(\n",
    "    df_population_by_country[\"Country\"] != \"World\"\n",
    ").select(\"Country\", \"1990\", \"2000\")\n",
    "\n",
    "# * Remove rows where either 1990 or 2000 is null\n",
    "df_population = df_population.filter(F.col(\"1990\").isNotNull() & F.col(\"2000\").isNotNull())\n",
    "\n",
    "\n",
    "df_population = df_population.withColumn(\n",
    "    \"Percentage Change\", ((F.col(\"2000\") - F.col(\"1990\")) / F.col(\"1990\")) * 100\n",
    ")\n",
    "max_increase = df_population.orderBy(F.col(\"Percentage Change\").desc()).limit(1)\n",
    "max_decrease = df_population.orderBy(F.col(\"Percentage Change\").asc()).limit(1)\n",
    "\n",
    "print(\"Country with the biggest percentage increase:\")\n",
    "max_increase.select(\"Country\", \"Percentage Change\").show()\n",
    "print(\"Country with the biggest percentage decrease:\")\n",
    "max_decrease.select(\"Country\", \"Percentage Change\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "|31678741|\n",
      "|       p|\n",
      "|       f|\n",
      "|promised|\n",
      "|      to|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 2925445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# * explode the array of words into individual rows and select only the word column\n",
    "df_words = df_text_tokenized.select(F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# * show the result\n",
    "df_words.show(5)\n",
    "print(f\"Total tokens: {df_words.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word|count |\n",
      "+----+------+\n",
      "|the |142965|\n",
      "|to  |87873 |\n",
      "|p   |78583 |\n",
      "|of  |75074 |\n",
      "|and |70933 |\n",
      "|in  |52844 |\n",
      "|a   |50187 |\n",
      "|for |28369 |\n",
      "|he  |27781 |\n",
      "|is  |27646 |\n",
      "|that|27443 |\n",
      "|s   |25354 |\n",
      "|on  |23636 |\n",
      "|are |19529 |\n",
      "|with|18699 |\n",
      "|be  |17764 |\n",
      "|as  |16110 |\n",
      "|have|16083 |\n",
      "|at  |15209 |\n",
      "|said|14893 |\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# * group by words and count the occurrences\n",
    "df_word_count = df_words.groupBy(\"word\").count().orderBy(F.col(\"count\").desc())\n",
    "\n",
    "df_word_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 Most Common Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 184:===================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|bigram |count|\n",
      "+-------+-----+\n",
      "|of the |17217|\n",
      "|in the |12045|\n",
      "|p he   |10876|\n",
      "|to the |8227 |\n",
      "|n t    |5368 |\n",
      "|for the|5328 |\n",
      "|on the |4806 |\n",
      "|to be  |4522 |\n",
      "|will be|4171 |\n",
      "|and the|3881 |\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# * use ngram to get the bigrams\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "df_bigrams = ngram.transform(df_text_tokenized)\n",
    "\n",
    "# * explode bigram token arrays into rows\n",
    "df_bigrams_exploded = df_bigrams.select(F.explode(F.col(\"bigrams\")).alias(\"bigram\"))\n",
    "\n",
    "# * group by bigram and count the occurrences \n",
    "df_bigrams_count = df_bigrams_exploded.groupBy(\"bigram\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# * show the top-10 bigram\n",
    "df_bigrams_count.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
